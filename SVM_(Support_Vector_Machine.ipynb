{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "def __init__(self, learning_rate=0.001, lambda_param=0.01, n_iters=1000):\n",
        "    self.lr = learning_rate\n",
        "    self.lambda_param = lambda_param\n",
        "    self.n_iters = n_iters\n",
        "    self.w = None\n",
        "    self.b = None\n",
        "This part initializes the main hyperparameters of the SVM model:\n",
        "\n",
        "learning_rate â†’ controls how much we update the model during training.\n",
        "\n",
        "lambda_param â†’ the regularization parameter that controls overfitting.\n",
        "\n",
        "n_iters â†’ number of iterations (epochs) for training.\n",
        "\n",
        "w and b â†’ model parameters (weights and bias), initially set to None until training starts.\n",
        "\n",
        "ğŸ“Œ Step 4: The fit() Method â€” Training the Model\n",
        "def fit(self, X, y):\n",
        "This function trains the SVM model using the training data.\n",
        "\n",
        "ğŸ”¹ Step 4.1: Get Dataset Dimensions\n",
        "n_samples, n_features = X.shape\n",
        "We extract:\n",
        "\n",
        "n_samples â†’ number of training examples\n",
        "\n",
        "n_features â†’ number of features in each example\n",
        "\n",
        "ğŸ”¹ Step 4.2: Convert Labels to -1 and 1\n",
        "y_ = np.where(y <= 0, -1, 1)\n",
        "SVM requires labels in the form of -1 and +1 (not 0 and 1), so we convert them here.\n",
        "\n",
        "ğŸ”¹ Step 4.3: Initialize Parameters\n",
        "self.w = np.zeros(n_features)\n",
        "self.b = 0\n",
        "We start with zero weights and zero bias.\n",
        "\n",
        "âš™ï¸ Step 5: The Learning Process (Gradient Descent)\n",
        "We iterate n_iters times, updating w and b based on how far each data point is from the optimal decision boundary.\n",
        "for _ in range(self.n_iters):\n",
        "    for idx, x_i in enumerate(X):\n",
        "        condition = y_[idx] * (np.dot(x_i, self.w) - self.b) >= 1\n",
        "Hereâ€™s whatâ€™s happening:\n",
        "\n",
        "We calculate whether each data point satisfies the margin condition\n",
        "(i.e., itâ€™s correctly classified and outside the margin).\n",
        "\n",
        "ğŸ§© If the condition is satisfied:\n",
        "self.w -= self.lr * (2 * self.lambda_param * self.w)\n",
        "Only apply regularization â€” no need to adjust bias.\n",
        "\n",
        "ğŸ§© If the condition is violated:\n",
        "self.w -= self.lr * (2 * self.lambda_param * self.w - np.dot(x_i, y_[idx]))\n",
        "self.b -= self.lr * y_[idx]\n",
        "We update both weights (w) and bias (b) to push the model to correctly classify the point.\n",
        "\n"
      ],
      "metadata": {
        "id": "PF-IZFpWOHl4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "va1XeabEOB67"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "class SVM:\n",
        "    def __init__(self, learning_rate=0.001, lambda_param=0.01, n_iters=1000):\n",
        "        self.lr = learning_rate\n",
        "        self.lambda_param = lambda_param\n",
        "        self.n_iters = n_iters\n",
        "        self.w = None\n",
        "        self.b = None\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        n_samples, n_features = X.shape\n",
        "        y_ = np.where(y <= 0, -1, 1)  # convert labels to -1, 1\n",
        "\n",
        "        self.w = np.zeros(n_features)\n",
        "        self.b = 0\n",
        "\n",
        "        for _ in range(self.n_iters):\n",
        "            for idx, x_i in enumerate(X):\n",
        "                condition = y_[idx] * (np.dot(x_i, self.w) - self.b) >= 1\n",
        "                if condition:\n",
        "                    self.w -= self.lr * (2 * self.lambda_param * self.w)\n",
        "                else:\n",
        "                    self.w -= self.lr * (2 * self.lambda_param * self.w - np.dot(x_i, y_[idx]))\n",
        "                    self.b -= self.lr * y_[idx]\n",
        "\n",
        "    def predict(self, X):\n",
        "        approx = np.dot(X, self.w) - self.b\n",
        "        return np.sign(approx)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ğŸ”¹ Explanation:\n",
        "\n",
        "We create a simple dataset with two features per sample (X) and labels -1 or 1 (y).\n",
        "\n",
        "We initialize the model and train it using fit().\n",
        "\n",
        "Then, we call predict(X) to see how the model classifies each point.\n",
        "\n",
        "ğŸ¯ Whatâ€™s Happening Behind the Scenes?\n",
        "\n",
        "The model tries to find the best hyperplane (a straight line in 2D) that separates the two classes with maximum margin.\n",
        "\n",
        "The â€œsupport vectorsâ€ are the closest points to this line â€” they define the margin boundaries.\n",
        "\n",
        "During training, the model adjusts w and b until the margin between classes is maximized and misclassifications are minimized.\n",
        "\n",
        "ğŸ§© Intuitive Summary\n",
        "\n",
        "SVM doesnâ€™t just draw any line â€” it finds the most confident line possible,\n",
        "the one that maximizes the distance between different classes.\n",
        "\n",
        "Thatâ€™s why itâ€™s called a â€œmaximum margin classifier.â€\n",
        "\n",
        "ğŸš€ Key Takeaways\n",
        "\n",
        "âœ… You now understand the core math behind SVM without using libraries like scikit-learn.\n",
        "âœ… This scratch implementation builds your intuition about how the algorithm actually learns.\n",
        "âœ… Once you understand this, using SVM in libraries becomes much clearer."
      ],
      "metadata": {
        "id": "dp35kenRPJxt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example dataset\n",
        "X = np.array([\n",
        "    [1, 2],\n",
        "    [2, 3],\n",
        "    [3, 3],\n",
        "    [2, 1],\n",
        "    [3, 2]\n",
        "])\n",
        "y = np.array([-1, -1, 1, 1, 1])\n",
        "\n",
        "# Train SVM model\n",
        "model = SVM(learning_rate=0.001, lambda_param=0.01, n_iters=1000)\n",
        "model.fit(X, y)\n",
        "\n",
        "# Predict\n",
        "predictions = model.predict(X)\n",
        "print(\"Predictions:\", predictions)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HlsC0QPiODlu",
        "outputId": "462c4354-c453-4716-a558-088145c699c0"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predictions: [-1. -1.  1.  1.  1.]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-NeWoltxOGmI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}