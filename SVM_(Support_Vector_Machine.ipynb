{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "def __init__(self, learning_rate=0.001, lambda_param=0.01, n_iters=1000):\n",
        "    self.lr = learning_rate\n",
        "    self.lambda_param = lambda_param\n",
        "    self.n_iters = n_iters\n",
        "    self.w = None\n",
        "    self.b = None\n",
        "This part initializes the main hyperparameters of the SVM model:\n",
        "\n",
        "learning_rate → controls how much we update the model during training.\n",
        "\n",
        "lambda_param → the regularization parameter that controls overfitting.\n",
        "\n",
        "n_iters → number of iterations (epochs) for training.\n",
        "\n",
        "w and b → model parameters (weights and bias), initially set to None until training starts.\n",
        "\n",
        "📌 Step 4: The fit() Method — Training the Model\n",
        "def fit(self, X, y):\n",
        "This function trains the SVM model using the training data.\n",
        "\n",
        "🔹 Step 4.1: Get Dataset Dimensions\n",
        "n_samples, n_features = X.shape\n",
        "We extract:\n",
        "\n",
        "n_samples → number of training examples\n",
        "\n",
        "n_features → number of features in each example\n",
        "\n",
        "🔹 Step 4.2: Convert Labels to -1 and 1\n",
        "y_ = np.where(y <= 0, -1, 1)\n",
        "SVM requires labels in the form of -1 and +1 (not 0 and 1), so we convert them here.\n",
        "\n",
        "🔹 Step 4.3: Initialize Parameters\n",
        "self.w = np.zeros(n_features)\n",
        "self.b = 0\n",
        "We start with zero weights and zero bias.\n",
        "\n",
        "⚙️ Step 5: The Learning Process (Gradient Descent)\n",
        "We iterate n_iters times, updating w and b based on how far each data point is from the optimal decision boundary.\n",
        "for _ in range(self.n_iters):\n",
        "    for idx, x_i in enumerate(X):\n",
        "        condition = y_[idx] * (np.dot(x_i, self.w) - self.b) >= 1\n",
        "Here’s what’s happening:\n",
        "\n",
        "We calculate whether each data point satisfies the margin condition\n",
        "(i.e., it’s correctly classified and outside the margin).\n",
        "\n",
        "🧩 If the condition is satisfied:\n",
        "self.w -= self.lr * (2 * self.lambda_param * self.w)\n",
        "Only apply regularization — no need to adjust bias.\n",
        "\n",
        "🧩 If the condition is violated:\n",
        "self.w -= self.lr * (2 * self.lambda_param * self.w - np.dot(x_i, y_[idx]))\n",
        "self.b -= self.lr * y_[idx]\n",
        "We update both weights (w) and bias (b) to push the model to correctly classify the point.\n",
        "\n"
      ],
      "metadata": {
        "id": "PF-IZFpWOHl4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "va1XeabEOB67"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "class SVM:\n",
        "    def __init__(self, learning_rate=0.001, lambda_param=0.01, n_iters=1000):\n",
        "        self.lr = learning_rate\n",
        "        self.lambda_param = lambda_param\n",
        "        self.n_iters = n_iters\n",
        "        self.w = None\n",
        "        self.b = None\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        n_samples, n_features = X.shape\n",
        "        y_ = np.where(y <= 0, -1, 1)  # convert labels to -1, 1\n",
        "\n",
        "        self.w = np.zeros(n_features)\n",
        "        self.b = 0\n",
        "\n",
        "        for _ in range(self.n_iters):\n",
        "            for idx, x_i in enumerate(X):\n",
        "                condition = y_[idx] * (np.dot(x_i, self.w) - self.b) >= 1\n",
        "                if condition:\n",
        "                    self.w -= self.lr * (2 * self.lambda_param * self.w)\n",
        "                else:\n",
        "                    self.w -= self.lr * (2 * self.lambda_param * self.w - np.dot(x_i, y_[idx]))\n",
        "                    self.b -= self.lr * y_[idx]\n",
        "\n",
        "    def predict(self, X):\n",
        "        approx = np.dot(X, self.w) - self.b\n",
        "        return np.sign(approx)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "🔹 Explanation:\n",
        "\n",
        "We create a simple dataset with two features per sample (X) and labels -1 or 1 (y).\n",
        "\n",
        "We initialize the model and train it using fit().\n",
        "\n",
        "Then, we call predict(X) to see how the model classifies each point.\n",
        "\n",
        "🎯 What’s Happening Behind the Scenes?\n",
        "\n",
        "The model tries to find the best hyperplane (a straight line in 2D) that separates the two classes with maximum margin.\n",
        "\n",
        "The “support vectors” are the closest points to this line — they define the margin boundaries.\n",
        "\n",
        "During training, the model adjusts w and b until the margin between classes is maximized and misclassifications are minimized.\n",
        "\n",
        "🧩 Intuitive Summary\n",
        "\n",
        "SVM doesn’t just draw any line — it finds the most confident line possible,\n",
        "the one that maximizes the distance between different classes.\n",
        "\n",
        "That’s why it’s called a “maximum margin classifier.”\n",
        "\n",
        "🚀 Key Takeaways\n",
        "\n",
        "✅ You now understand the core math behind SVM without using libraries like scikit-learn.\n",
        "✅ This scratch implementation builds your intuition about how the algorithm actually learns.\n",
        "✅ Once you understand this, using SVM in libraries becomes much clearer."
      ],
      "metadata": {
        "id": "dp35kenRPJxt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example dataset\n",
        "X = np.array([\n",
        "    [1, 2],\n",
        "    [2, 3],\n",
        "    [3, 3],\n",
        "    [2, 1],\n",
        "    [3, 2]\n",
        "])\n",
        "y = np.array([-1, -1, 1, 1, 1])\n",
        "\n",
        "# Train SVM model\n",
        "model = SVM(learning_rate=0.001, lambda_param=0.01, n_iters=1000)\n",
        "model.fit(X, y)\n",
        "\n",
        "# Predict\n",
        "predictions = model.predict(X)\n",
        "print(\"Predictions:\", predictions)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HlsC0QPiODlu",
        "outputId": "462c4354-c453-4716-a558-088145c699c0"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predictions: [-1. -1.  1.  1.  1.]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-NeWoltxOGmI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}